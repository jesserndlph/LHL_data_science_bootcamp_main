{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines and Model Persistence - W07D1\n",
    "### Instructor: Eric Elmoznino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview - Persistence\n",
    "- Motivation\n",
    "- Pickle\n",
    "- Joblib\n",
    "- Saving parameters only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Motivation\n",
    "Serialization is the process of converting a program entity into a stream of bytes that can be saved as a file.\n",
    "There are two primary reasons why we might want to save (and later load) a trained model:\n",
    "- Avoid redundant training. Models can take a long time to train and data can take a long time to load/process\n",
    "- Deployment into an application (keep model training/deployment code separate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pickle\n",
    "- Pickling is the process where a Python object is converted into a byte stream (usually not human readable).\n",
    "- Unpickling is the reverse operation, where a byte stream is converted back into a working Python object.\n",
    "- Pickling is the simplest way to store the object from a coding perspective.\n",
    "- The Python Pickle module is an object-oriented way of storing objects.\n",
    "    - It can store *any* Python object, not just Sklearn models.\n",
    "    \n",
    "#### Features\n",
    "- Store/load dictionaries and lists.\n",
    "- Store/load the attributes of arbitrary data types (i.e. classes)\n",
    "- Do this recursively, so that if your object has attributes that are\n",
    "classes themselves, it can be saved just as easily\n",
    "\n",
    "#### Limitations\n",
    "- Does not save the *code* of an object — only its attribute values.\n",
    "- Cannot save file handles or connection sockets.\n",
    "- Pickle is **version-dependent**. For example, if you saved a model with a certain version\n",
    "of Sklearn then try to load it with a different one (e.g. you updated), there may be issues.\n",
    "    - Another motivation for using virtual environments, which can be containerized.\n",
    "\n",
    "#### Saving procedure\n",
    "```python\n",
    "import pickle        # Built-in python module\n",
    "\n",
    "# Create some object and manipulate it in some way (e.g. train the model)\n",
    "myobj = SomeClass(...)\n",
    "myobj = myobj.some_method(...)\n",
    "\n",
    "# Save to a file using Pickle\n",
    "with open('myfile.pickle', 'wb') as file_handle:\n",
    "    pickle.dump(myobj, file_handle)\n",
    "```\n",
    "\n",
    "#### Loading procedure\n",
    "```python\n",
    "import pickle        # Built-in python module\n",
    "\n",
    "# Load from a file using Pickle\n",
    "with open('myfile.pickle', 'rb') as file_handle:\n",
    "    myobj = pickle.load(file_handle)    # myobj will be an instance of SomeClass\n",
    "```\n",
    "\n",
    "#### Methods\n",
    "The pickle module provides four different methods:\n",
    "- dump() − The dump() method serializes to an open file (file-like object).\n",
    "- dumps() − Serializes to a string.\n",
    "- load() − Deserializes from an open-like object.\n",
    "- loads() − Deserializes from a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaling', StandardScaler()), ('pca', PCA(n_components=3)),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv(url, names=names)\n",
    "X = df.drop(columns='class')\n",
    "y = df['class']\n",
    "\n",
    "pipeline = Pipeline(steps=[('scaling', StandardScaler()),\n",
    "                           ('pca', PCA(n_components=3)),\n",
    "                           ('classifier', LogisticRegression())])\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open('saved_models/pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Load the model\n",
    "with open('saved_models/pipeline.pickle', 'rb') as f:\n",
    "    pipeline_loaded = pickle.load(f)\n",
    "\n",
    "assert (pipeline.steps[2][1].coef_ == pipeline_loaded.steps[2][1].coef_).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Joblib\n",
    "Joblib is an alternative serialization module to Pickle. It's main advantage over Pickle\n",
    "is that it is faster and more efficient at saving large `numpy` arrays.\n",
    "\n",
    "<sub>*Note: Starting with Python 3.8, Pickle is actually better than Joblib for saving `numpy` arrays.\n",
    "    If you have Python >=3.8, just use Pickle. [Source](https://stackoverflow.com/a/12617603).*</sub>\n",
    "    \n",
    "#### Saving procedure\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Create some object and manipulate it in some way (e.g. train the model)\n",
    "myobj = SomeClass(...)\n",
    "myobj = myobj.some_method(...)\n",
    "\n",
    "# Save to a file using Joblib\n",
    "joblib.dump(myobj, file_path)\n",
    "```\n",
    "\n",
    "#### Loading procedure\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load from a file using Joblib\n",
    "myobj = joblib.load(file_path)    # myobj will be an instance of SomeClass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipeline, 'saved_models/pipeline.can')\n",
    "\n",
    "# Load the model\n",
    "pipeline_loaded = joblib.load('saved_models/pipeline.can')\n",
    "    \n",
    "assert (pipeline.steps[2][1].coef_ == pipeline_loaded.steps[2][1].coef_).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving parameters only\n",
    "Oftentimes, we don't want to save the entire Python class. What we really might want to save\n",
    "could just be the model parameters. This can have some advantages:\n",
    "- Faster to save, smaller data size, and simpler storage format, since parameters are usually just numpy arrays.\n",
    "- No version issues, since you are just saving numbers. If the Sklearn developers change something\n",
    "about the model class, you will still be able to safely load the parameters you saved.\n",
    "- Framework-independent (i.e. don't have to load back in Sklearn, or even Python).\n",
    "\n",
    "Of course, there are also some disadvantages:\n",
    "- More convoluted process programmatically\n",
    "- Models might have many different parameter attributes\n",
    "- Models might have complex architectures that link their parameters\n",
    "- You cannot save an entire workflow (e.g. Sklearn pipeline)\n",
    "- When loading, you have to recreate the model and define it the same way as when you trained it,\n",
    "before restoring the parameters\n",
    "\n",
    "#### Saving procedure\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Create some object and manipulate it in some way (e.g. train the model)\n",
    "myobj = SomeClass(param1=a, param2=b, ...)\n",
    "myobj = myobj.some_method(...)\n",
    "\n",
    "# Save parameters to a file using numpy\n",
    "np.save(file_path, myobj.some_parameters)\n",
    "np.save(...)    # Save all the different parameters (e.g. weights and intercept) separately\n",
    "```\n",
    "\n",
    "#### Loading procedure\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Recreate the model exactly as you did when training it\n",
    "myobj = SomeClass(param1=a, param2=b, ...)\n",
    "\n",
    "# Load parameters from a file using numpy\n",
    "params = numpy.load(file_path)\n",
    "\n",
    "# Replace the model's parameters\n",
    "myobj.some_parameters = params\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "model = RidgeClassifier(alpha=0.1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the model's parameters\n",
    "np.save('saved_models/logistic_coef.npy', model.coef_)\n",
    "np.save('saved_models/logistic_intercept.npy', model.intercept_)\n",
    "\n",
    "# Recreate the model with the same arguments as when you trained it\n",
    "loaded_model = RidgeClassifier(alpha=0.1)\n",
    "\n",
    "# Load the model's parameters\n",
    "loaded_model.coef_ = np.load('saved_models/logistic_coef.npy')\n",
    "loaded_model.intercept_ = np.load('saved_models/logistic_intercept.npy')\n",
    "    \n",
    "assert (model.coef_ == loaded_model.coef_).all()\n",
    "assert (model.intercept_ == loaded_model.intercept_).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
