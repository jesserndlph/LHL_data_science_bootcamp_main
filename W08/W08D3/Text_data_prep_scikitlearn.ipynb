{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e446943",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f9ef0",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551a0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['i', 'superb,', 'in', 'am', 'hate', 'love', 'phone', 'this']\n",
      "vectors:  [[1, 1, 1, 1, 0, 1, 1, 1], [1, 0, 0, 0, 1, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \"I hate this phone\"]\n",
    "words = list(set([word for doc in docs for word in doc.lower().split()]))\n",
    "vectors = []\n",
    "for doc in docs:\n",
    "    vectors.append([1 if word in doc.lower().split() else 0 for word in words])\n",
    "print(\"vocabulary: \", words)   \n",
    "print(\"vectors: \", vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831b106",
   "metadata": {},
   "source": [
    "### Word counts with CountVectorizer(scikitlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7616171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "shape:  (2, 7)\n",
      "vectors:  [[1 0 2 1 1 1 1]\n",
      " [0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(docs)\n",
    "# summarize encoded vector\n",
    "print('shape: ', vector.shape)\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0251fe",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d893a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "idfs:  [1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.        ]\n",
      "vectors:  [[0.35327777 0.         0.70655553 0.35327777 0.25136004 0.35327777\n",
      "  0.25136004]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of documents\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \n",
    "        \"I hate this phone\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "# summarize\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "print('idfs: ', vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([docs[0]])\n",
    "# summarize encoded vector\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650488ff",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025b2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# train Data\n",
    "trainData = pd.read_csv(\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\")\n",
    "# test Data\n",
    "testData = pd.read_csv(\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6718205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>anna and the king strides onto the screen in f...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>in the grand scheme of mel gibson movies , pay...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>the corruptor is a big silly mess of an action...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>hello kids . \\ntoday the movie studios want to...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>jamaica is a hot vacation spot . \\nthe exchang...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content Label\n",
       "287   anna and the king strides onto the screen in f...   pos\n",
       "530   in the grand scheme of mel gibson movies , pay...   pos\n",
       "1526  the corruptor is a big silly mess of an action...   neg\n",
       "1204  hello kids . \\ntoday the movie studios want to...   neg\n",
       "885   jamaica is a hot vacation spot . \\nthe exchang...   pos"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.sample(frac=1).head(5) \n",
    "# shuffle the df and pick first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a19d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(trainData['Content'])\n",
    "test_vectors = vectorizer.transform(testData['Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38002d",
   "metadata": {},
   "source": [
    "### Creating a linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be2d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.296200s; Prediction time: 0.516672s\n",
      "positive:  {'precision': 0.9191919191919192, 'recall': 0.91, 'f1-score': 0.9145728643216081, 'support': 100}\n",
      "negative:  {'precision': 0.9108910891089109, 'recall': 0.92, 'f1-score': 0.9154228855721394, 'support': 100}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, trainData['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(testData['Label'], prediction_linear, output_dict=True)\n",
    "print('positive: ', report['pos'])\n",
    "print('negative: ', report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6801c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos']\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"SUPERB, I AM IN LOVE IN THIS PHONE\"\"\"\n",
    "review_vector = vectorizer.transform([review]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ceb3c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Do not purchase this product. My cell phone blast when I switched the charger\"\"\"\n",
    "review_vector = vectorizer.transform([review]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033c4405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"I received defective piece display is not working properly\"\"\"\n",
    "review_vector = vectorizer.transform([review]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2ebca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"It's not even 5 days since i purchased this product.\n",
    "I would say this a specially blended worst Phone in all formats.\n",
    "ISSUE 1:\n",
    "Have you ever heard of phone which gets drained even in standby mode during night?\n",
    "Kindly please see the screenshot if you want to believe my statement.\n",
    "My phone was in full charge at night 10:07 PM . I took this screenshot and went to sleep.\n",
    "Then I woke up at morning and 6:35 AM and battery got drained by 56% in just standby condition.\n",
    "If this is the case consider how many hours it will work, during day time.\n",
    "It's not even 5 hours the battery is able to withstand.\n",
    "ISSUE 2:\n",
    "Apart from the battery, the next issue is the heating issue .I purchased a iron box recently from Bajaj in this sale.\n",
    "But I realized this phone acts a very good Iron box than the Bajaj Iron box. I am using only my headphones to get connected in the call. I am not sure when this phone is will get busted due to this heating issue. It is definitely a challenge to hold this phone for even 1 minute. The heat that the phone is causing will definitely burn your hands and for man if you keep this phone in your pant pocket easily this will lead to infertility for you. Kindly please be aware about that.\n",
    "Issue 3:\n",
    "Even some unknown brands has a better touch sensitivity. The touch sensitivity is pathetic, if perform some operation it will easily take 1-2 minutes for the phone to response.\n",
    "For your kind information my system has 73% of Memory free and the RAM is also 56% free.\n",
    "Kindly please make this Review famous and lets make everyone aware of this issue with this phone.\n",
    "Let's save people from buying this phone. There are people who don't even know what to do if this issue happens after 10 days from the date of purchase. So I feel at least this review will help people from purchasing this product in mere future.\"\"\"\n",
    "review_vector = vectorizer.transform([review]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e976a91",
   "metadata": {},
   "source": [
    "### Pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00825c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickling the vectorizer\n",
    "pickle.dump(vectorizer, open('vectorizer.sav', 'wb'))\n",
    "# pickling the model\n",
    "pickle.dump(classifier_linear, open('classifier.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b938b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3613jvsc74a57bd07257430773ff2786ada9528d4be6e0e9fe11a1482846f09bd681c7e1b20b89d3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
